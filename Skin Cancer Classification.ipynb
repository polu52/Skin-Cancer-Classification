{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8925a69-1125-462d-9416-7640f73fd546",
   "metadata": {},
   "source": [
    "# Skin Cancer Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70b72e-4e2d-47f6-a92d-ca8999ed1621",
   "metadata": {},
   "source": [
    "In this project, we will create a classification model using skin cancer images labeled as cancer or non-cancer obtained from Kaggle.com.We will use Keras deep learning. Then we will save our model. We will integrate the model into our website that we prepared with Streamlit library. When we give a new skin photo, the model will predict cancer or not it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff840b-ef0f-4894-9d8f-8bc00fe6d557",
   "metadata": {},
   "source": [
    "<img src='https://news.christianacare.org/wp-content/uploads/2024/05/GettyImages-1404462398.jpg' >\n",
    "<a href='https://www.kaggle.com/datasets/kylegraupe/skin-cancer-binary-classification-dataset'>Click her to reach dataset<a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f669ddd5-0704-4087-a420-f7db75ada8f1",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787e7e08-63f9-4014-b140-7b4b60d1b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c36e8d-702a-4e07-af0f-5618e3af801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5054283a-38cb-4c56-b4f1-18b45fa9d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['Cancer','Non_Cancer']\n",
    "img_path='Skin_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f103242b-93c3-4091-ac4d-aa42ebef3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list=[]\n",
    "label_list=[]\n",
    "for label in labels:\n",
    "    for img_file in os.listdir(img_path+label):\n",
    "        img_list.append(img_path+label+'/'+img_file)\n",
    "        label_list.append(label)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156345f0-5e03-4ae0-874b-91d8dc66948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'img':img_list,'label':label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c44e93a-861c-4a02-a8e8-b272d3f7700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skin_Data/Cancer/1007-1.jpg</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skin_Data/Cancer/1010-01.JPG</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skin_Data/Cancer/1012-2.JPG</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skin_Data/Cancer/1031-1.jpg</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skin_Data/Cancer/1051-3(94).jpg</td>\n",
       "      <td>Cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               img   label\n",
       "0      Skin_Data/Cancer/1007-1.jpg  Cancer\n",
       "1     Skin_Data/Cancer/1010-01.JPG  Cancer\n",
       "2      Skin_Data/Cancer/1012-2.JPG  Cancer\n",
       "3      Skin_Data/Cancer/1031-1.jpg  Cancer\n",
       "4  Skin_Data/Cancer/1051-3(94).jpg  Cancer"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14c44cc-fe2d-4aa5-bb8c-f875dadf65a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Skin_Data/Non_Cancer/953-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Skin_Data/Non_Cancer/954-3.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Skin_Data/Non_Cancer/955.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Skin_Data/Non_Cancer/984.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Skin_Data/Non_Cancer/986-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                img       label\n",
       "283  Skin_Data/Non_Cancer/953-1.JPG  Non_Cancer\n",
       "284  Skin_Data/Non_Cancer/954-3.JPG  Non_Cancer\n",
       "285    Skin_Data/Non_Cancer/955.JPG  Non_Cancer\n",
       "286    Skin_Data/Non_Cancer/984.JPG  Non_Cancer\n",
       "287  Skin_Data/Non_Cancer/986-1.JPG  Non_Cancer"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33877257-fcd3-4226-8471-98789eb104bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0032d7d4-c5ce-4c46-89e3-7c7cef546b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'Cancer':1,'Non_Cancer':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5abaf3f2-cd50-4202-ae58-0ad69ee88959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encode_label']=df['label'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be7b5d2f-ecd8-435e-af6e-2e149b5d0e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>encode_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Skin_Data/Non_Cancer/953-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Skin_Data/Non_Cancer/954-3.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Skin_Data/Non_Cancer/955.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Skin_Data/Non_Cancer/984.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Skin_Data/Non_Cancer/986-1.JPG</td>\n",
       "      <td>Non_Cancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                img       label  encode_label\n",
       "283  Skin_Data/Non_Cancer/953-1.JPG  Non_Cancer             0\n",
       "284  Skin_Data/Non_Cancer/954-3.JPG  Non_Cancer             0\n",
       "285    Skin_Data/Non_Cancer/955.JPG  Non_Cancer             0\n",
       "286    Skin_Data/Non_Cancer/984.JPG  Non_Cancer             0\n",
       "287  Skin_Data/Non_Cancer/986-1.JPG  Non_Cancer             0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d433e58d-b435-4b8b-8214-5a284396cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a64c96-1891-4a06-8b31-38268120df01",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef22c8a-2d70-4a82-9c3f-0748c5186192",
   "metadata": {},
   "source": [
    "We will identify x and y datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34cb3ac-2673-472f-b660-4bf8fe3f50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for img in df['img']:\n",
    "    img=cv2.imread(img)\n",
    "    img=cv2.resize(img,(170,170)) #boyutunu 170x170 pixel yaptık\n",
    "    img=img/255.0 #normalize ettik\n",
    "    x.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7853749f-377c-46d8-8467-fca6e6a49f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc90361e-11db-4090-9005-4007b149fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['encode_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41b673-a61b-41ce-a546-1608a2f9149f",
   "metadata": {},
   "source": [
    "Let's import train test split and we will split%20 of our datas as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "211c4a90-73f7-48b3-9f2f-4ce3419b258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7ff1f0-6196-4c71-a62d-589e04edae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ccb7c71-6874-4e05-99b1-50f31d6f4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D,Dropout,BatchNormalization,Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824f8fd-690c-4d69-90cb-308c86ad0a2c",
   "metadata": {},
   "source": [
    "CNN - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb51c6f3-5b6b-43bc-b3bc-662d0ce65c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(170,170,3)))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c71a9370-8304-47fb-9d27-4bcaf228f13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.5806 - loss: 11.8310 - val_accuracy: 0.7414 - val_loss: 0.6951\n",
      "Epoch 2/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - accuracy: 0.7215 - loss: 0.5766 - val_accuracy: 0.7586 - val_loss: 0.6118\n",
      "Epoch 3/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 216ms/step - accuracy: 0.7379 - loss: 0.5827 - val_accuracy: 0.7069 - val_loss: 0.5702\n",
      "Epoch 4/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - accuracy: 0.7514 - loss: 0.5417 - val_accuracy: 0.7759 - val_loss: 0.4773\n",
      "Epoch 5/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - accuracy: 0.7482 - loss: 0.4794 - val_accuracy: 0.7931 - val_loss: 0.4672\n",
      "Epoch 6/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 223ms/step - accuracy: 0.8129 - loss: 0.4089 - val_accuracy: 0.7931 - val_loss: 0.4456\n",
      "Epoch 7/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - accuracy: 0.8519 - loss: 0.3539 - val_accuracy: 0.8103 - val_loss: 0.4855\n",
      "Epoch 8/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.8693 - loss: 0.3526 - val_accuracy: 0.8276 - val_loss: 0.3977\n",
      "Epoch 9/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - accuracy: 0.9207 - loss: 0.2795 - val_accuracy: 0.7241 - val_loss: 0.5207\n",
      "Epoch 10/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 224ms/step - accuracy: 0.9100 - loss: 0.2744 - val_accuracy: 0.9138 - val_loss: 0.3301\n",
      "Epoch 11/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - accuracy: 0.9506 - loss: 0.1710 - val_accuracy: 0.8966 - val_loss: 0.2993\n",
      "Epoch 12/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - accuracy: 0.9600 - loss: 0.1441 - val_accuracy: 0.8621 - val_loss: 0.4662\n",
      "Epoch 13/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.9392 - loss: 0.1585 - val_accuracy: 0.8276 - val_loss: 0.4308\n",
      "Epoch 14/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 220ms/step - accuracy: 0.9312 - loss: 0.1530 - val_accuracy: 0.8621 - val_loss: 0.3114\n",
      "Epoch 15/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 219ms/step - accuracy: 0.9758 - loss: 0.0977 - val_accuracy: 0.9138 - val_loss: 0.2728\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs=15,validation_data=(x_test,y_test),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "902a8cda-1f56-4eb6-be1d-5de22d743fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('my_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe0fca-7285-4308-aae2-752f6330ab1b",
   "metadata": {},
   "source": [
    "We trained and saved our model.Now we can use it in any algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e69dc11-902b-40d4-ae5a-0c54f5c9579a",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727a79b-2e5b-44ad-b5aa-0c6c4fcc7294",
   "metadata": {},
   "source": [
    "We trained a new model from scratch using CNN. However, we can also achieve our goal by using pre-trained models and providing them with our own data. Among the pre-trained models, the most widely used in the field of image processing are the ResNet50 and VGG16 models. Here, we will use the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7cc9a45-4745-4641-931b-be00603924ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input, MaxPooling2D,Dropout,BatchNormalization,Reshape\n",
    "from tensorflow.keras.applications import VGG16,ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #tek satırda resim okuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f871816f-3205-4a33-987f-c8e9a090f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232 images belonging to 2 classes.\n",
      "Found 56 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polu5\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4s/step - accuracy: 0.7078 - loss: 5.1778 - val_accuracy: 0.2857 - val_loss: 1.7129\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 4s/step - accuracy: 0.5554 - loss: 1.3742 - val_accuracy: 0.7143 - val_loss: 1.1237\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 4s/step - accuracy: 0.6960 - loss: 0.7738 - val_accuracy: 0.8393 - val_loss: 0.4752\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4s/step - accuracy: 0.7899 - loss: 0.4413 - val_accuracy: 0.8393 - val_loss: 0.4198\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 4s/step - accuracy: 0.8361 - loss: 0.3825 - val_accuracy: 0.8214 - val_loss: 0.4013\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 4s/step - accuracy: 0.8443 - loss: 0.3397 - val_accuracy: 0.8571 - val_loss: 0.3820\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5s/step - accuracy: 0.8925 - loss: 0.3082 - val_accuracy: 0.8571 - val_loss: 0.3755\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 4s/step - accuracy: 0.9233 - loss: 0.2723 - val_accuracy: 0.8571 - val_loss: 0.3621\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 4s/step - accuracy: 0.8652 - loss: 0.2499 - val_accuracy: 0.8571 - val_loss: 0.3578\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 4s/step - accuracy: 0.9460 - loss: 0.2318 - val_accuracy: 0.8393 - val_loss: 0.3539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x26cc3d3dc70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir='Skin_Data'\n",
    "img_width,img_heigth=224,224\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1/255, validation_split=.20)\n",
    "\n",
    "train_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_heigth),\n",
    "                                class_mode='binary', subset='training')\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1/255)\n",
    "test_datagenerator=train_datagen.flow_from_directory(directory=data_dir,target_size=(img_width,img_heigth),\n",
    "                                class_mode='binary', subset='validation')\n",
    "\n",
    "base_model=VGG16(weights='imagenet', input_shape=(img_width,img_heigth,3),include_top=False)\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_datagenerator,epochs=10,validation_data=test_datagenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f859c76-180b-407b-9049-c19cf64faf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">25,691,136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │    \u001b[38;5;34m25,691,136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,025\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">91,791,173</span> (350.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m91,791,173\u001b[0m (350.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,692,161</span> (98.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,692,161\u001b[0m (98.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,384,324</span> (196.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m51,384,324\u001b[0m (196.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a3ef22d-39f0-4441-b263-1bd1b4343ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('my_cnn_tf_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa9662-4952-4842-b94a-d31a93dadd1a",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19aeb16-6754-4a9e-942f-947bacb2581a",
   "metadata": {},
   "source": [
    "We turned the file paths of the skin cancer images obtained from Kaggle.com into a DataFrame with the help of the Pandas and OS libraries. Then, we sequentially read all the images using the OpenCV library and normalized them for easier processing. Afterward, we trained our model using Keras deep learning. As a second step, we used the VGG16 model by feeding our data into it, training the pre-trained model with our data, and making it suitable for our purpose. Both methods can be used for Image Classification, but since training a model from scratch is a time-consuming process, the common approach is to use a pre-trained model and apply transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0739fb2-80ec-446c-a74e-a9af90151df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
